# Flakiness Predicter

This is a step-by-step guideline to use/replicate our classification experiment of FlakeFlagger.   

## Requirements
This is a list of all required python packages to run this part:
- Python >=3.6
- imbalanced_learn >= 0.6.2
- numpy >= 1.18.1
- pandas >= 1.0.1
- scikit_learn >= 0.22.1


## Input Files:
This is a list of input files that are required to accomplish this step:
* `result/processed_data.csv`: 
	This contains our collected features per test generated from `test-feature-collector`. 
* `input_data/original_tests/` :
	This directory contains the body contents of all flaky and non flaky tests. This is used to collect the token/java keywords per test. 

## How to replicate our experiment?
There are three main steps to perform the prediction phase. 
	* Hint: the first two steps can be skipped as the generated files from these steps are already in the `result` directory. 

1. Extend the processed_data.csv to contain the tokens and java keywords for each test. This is reuiqred as our main evaluation (Table III in our paper) contains the vocabulary_based_approach (where the classification is only based on the tokens and java keywords collected from each test). The output of this step is already collected and generated as shown in `result/processed_data_with_vocabulary_per_test.csv`. However, it can be re-generated by run:

```console
bash collectTokens.sh
```
This takes the `result/processed_data.csv` and `input_data/original_tests/` as inputs where the output will override `result/processed_data_with_vocabulary_per_test.csv`


2. Next, the features selection in our classification is based on information gain. To do that, we need to calculate the information gain for each token, java keyword, and FlakeFlagger feature in order to complete the classification. This can be done by run:

```console
bash informationGain.sh
```
This will re-generate the file `result/Information_gain_per_feature.csv`. Please note that this could take several hours to compute all information gain per each features/token (if we consider that there are more than 30k tokens in our provided dataset).

3. The third step is to build the model based on three approaches which are:
- Evaluation based on FlakeFlagger features only 
- Evaluation based on vocabualry based approach only
- Evaluation based on FlakeFlagger features and vocabualry based approach (merge approach)

running the following :
```console
bash predict.sh
``` 

will build the prediciation models based on the previous three approaches and perform the classification on our dataset. The input of this step are shown in `predict.sh`, where the output will be under `result/classification_result/` directory which contains three files:
1. `prediction_result.csv`: this shows the overall result based on the given selection of arguments.
2. `prediction_result_per_test`: list of all False positive, False negative, and True positive, for each result.
2. `prediction_result_by_project`: the confusion matrix split by project (similar to Table III in the paper)


* Note: We used minimum IG = 0.01, StratifiedKFold, Random Forest, 5000 as minimum number of trees,SMOTE as a balance technique, and 10% as split size. You can change the following arguments to explore the full classification result. The strucutre of these arguments comes as a list ( it could be one selection or a combination of multplie selection as a list) These arguments (shown in `predict.sh`) are:

1. Classifier: RF, SVM, DT, MLP, NB or KNN 
2. Fold types: StratifiedKFold and KFold 
3. Balance : SMOTE, undersampling and none (to see the result without balancing)
4. Any number of trees, IG, or split size



