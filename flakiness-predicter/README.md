# Flakiness Predicter

This is a step-by-step guideline to use/replicate our classification experiment of flaky tests.   

## Requirements
This is a list of all required python packages to run this part:
- Python >=3.6
- imbalanced_learn >= 0.6.2
- numpy >= 1.18.1
- pandas >= 1.0.1
- scikit_learn >= 0.22.1


## Input Files:
This is a list of input files that are required to accomplish this step:
* `result/processed_data.csv`: 
	This contains our collected features per test generated from `test-feature-collector`. 
* `input_data/original_tests/` :
	This directory contains the body contents of all flaky and non flaky tests. This is used to collect the token/java keywords per test. 

## Choice 1: How to replicate our experiment?
There are three main steps to perform the prediction phase. 
* Hint: the first two steps can be skipped as the generated files from these steps are already in the `result` directory. 

1. Extend the processed_data.csv to contain the tokens and java keywords for each test. This is reuiqred as our main evaluation (Table III in our paper) contains the vocabulary_based_approach (where the classification is only based on the tokens and java keywords collected from each test). The output of this step is already collected and generated as shown in `result/processed_data_with_vocabulary_per_test.csv`. However, it can be re-generated by run:

```console
bash collectTokens.sh
```
This takes the `result/processed_data.csv` and `input_data/original_tests/` as inputs where the output result will override `result/processed_data_with_vocabulary_per_test.csv`


2. Next, the features selection in our classification is based on information gain. To do that, we need to calculate the information gain for each token, java keyword, and FlakeFlagger feature in order to complete the classification. This can be done by run:

```console
bash informationGain.sh
```
This will re-generate the file `result/Information_gain_per_feature.csv`. Please note that this could take several hours to compute the information gain for each features/token (if we consider that there are more than 30k tokens in our provided flaky and non flaky tests).

3. The third step is to build the model based on three approaches which are:
- Evaluation based on FlakeFlagger features only 
- Evaluation based on vocabualry based approach only
- Evaluation based on FlakeFlagger features and vocabualry based approach (merge approach)

running the following :
```console
bash predict.sh
``` 

will build the prediciation models based on the previous three approaches and perform the classification on our dataset. The input of this step are shown in `predict.sh` as follow:
- `result/processed_data_with_vocabulary_per_test.csv`: the extended version of `processed_data.csv` which includes tokens/vocabualry per test.
- `input_data/FlakeFlaggerFeaturesTypes.csv`: the types of flakeflagger features (either static or dynamic).
- `result/Information_gain_per_feature.csv`: the computed information gain for each feature.
- `result/processed_data.csv`: the original collected data from `test-feature-collector`. This is used to map each test to its project for final summary. 

where the output will be under `result/classification_result/` directory which contains three files:
- `prediction_result.csv`: this shows the overall result based on the given selection of arguments.
- `prediction_result_per_test`: list of all False positive, False negative, and True positive, for each result.
- `prediction_result_by_project`: the confusion matrix split by project (similar to Table III in the paper)


** Note: We used `minimum IG = 0.01`, `StratifiedKFold` as cross validation type, `Random Forest (RF)` as a classifier, `5000` as minimum number of trees, `SMOTE` as a balance technique, and `10%` as a number of folds. You can change the following arguments to explore the full classification result. The strucutre of these arguments comes as a list (it could be a list of size one argument or more than that). These arguments (shown in `flakeflagger_predicter.py`) are:

	1. Classifier: `RF`, `SVM`, `DT`, `MLP`, `NB`, `KNN` 
	2. Fold types: `StratifiedKFold` and `KFold`
	3. Balance : `SMOTE`, `undersampling` and `none`(to see the result without balancing)
	4. Number of folds: `5` or `10`
	5. Minimum information gain: any value between `0` and `1`
	6. Number of Tree (for RF): `100-5000`
	


## Choice 2: How to apply FlakeFlagger predictor on your data?
If you have a dataset of flaky and non-flaky tests and you were able to run FlakeFlagger feature collector, all you need is to generate `processed_data.csv` and apply FlakeFlagger predictor. To do so, please follow the following steps.

1. run `generate_processed_data.py` on the directory of projects contains projects. These projects should have all csv files generated from the feature collection phase. Please refer to `test-feature-collector` on how to collect features from a set of java projects. `generate_processed_data.py` script takes three arguments which are:
	- `projects_dir`: it is the head directory which contains all projects where we collect features from.
	- `source_of_flaky_tests`: this is a csv file that should contain a list of flaky test names
	- `column_name`: it is the name of the column where flaky tests belong.
For example, if the directory of the java projects is `~/Desktop/projects/`, the csv file of flaky tests is `~/Desktop/flaky_test_list.csv`, and the name of the column in this csv file is `test_name`, then the command to generate `processed_data.csv`

```console
python generate_processed_data.py ~/Desktop/projects/ ~/Desktop/flaky_test_list.csv test_name
``` 

2. run the FlakeFlagger predictor script `extended_flakeflagger_predicter.py` as the following:

```console
python extended_flakeflagger_predicter.py result/your_processed_data.csv result/your_processed_data.csv
```  

The `result` should contain the prediction result. Please refer to the last hint in `Choise 1: How to replicate our experiment?` section to manipulate different classification arguments. 

** Note: using `result/your_processed_data.csv` means we do not use an external dataset for testing (the same we did in our paper by using the cross-validation). In fact, this script is capable of having an external dataset for testing. However, we encourage you to refer to the paper as we discuss that the flaky tests from `processed_data.csv` are not representing all flaky tests in java projects. This means the FlakeFlagger model may not perform well on external dataset. 
