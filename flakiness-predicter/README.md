# Flakiness Predicter

This is a simple step-by-step guidelines to use/replicate our classification experiment of FlakeFlagger.   

## Requirements
This is a list of all required python packages to run this part:
- Python >=3.6
- imbalanced_learn >= 0.6.2
- numpy >= 1.18.1
- pandas >= 1.0.1
- scikit_learn >= 0.22.1


## Input Files:
The input of this step are collected from the previous step, which are:
- result/processed_data.csv: 
	This contains our collected features per test generated from 'test-feature-collector'. 
- Input_data/original_tests/ :
	This directory contains the body content of all flaky and non flaky tests. This is used to collect the token/java keywords per test. 

## How to replicate our experiment?
There are three main steps to perform the prediction phase:

1. Extend the processed_data.csv to contain the tokens and java keywords for each test. This is reuiqred as our main evaluation (Table III in our paper) contains the vocabulary_based_approach (where the classification is only based on the tokens and java keywords collected from each test). The output of this step is already collected and generated as shown in result/processed_data_with_vocabulary_per_test.csv. However, it can be generated by run:

```console
bash collectTokens.sh
```
This takes the result/processed_data.csv and Input_data/original_tests/ as inputs where the output will override result/processed_data_with_vocabulary_per_test.csv


2. The features selection in our classification is based on information gain. To do that, we need to calculate the information gain for each token, java keyword, and FlakeFlagger feature in order to complete the classification. This can be done by run:

```console
bash informationGain.sh
```
This will re-generate the file result/Information_gain_per_feature.csv ---- add hint here .. 

3. The third step is to build the model based on three approaches which are:
- Evaluation based on FlakeFlagger features only 
- Evaluation based on vocabualry based approach only
- Evaluation based on FlakeFlagger features and vocabualry based approach (merge approach)

running the following :
```console
bash predict.sh
```
will builds prediciation models based on the previous three approaches and perform the classification. The inputs are:
- processed_data.csv .. 
- 
 
where the output will be under result/classification_result/ directory which contains three files:
1 summary classifications:  
2.list of all False positive, False negative, and True positive, for each result.
3. ( Table III )

Note: We used minimum IG = 0.01, StratifiedKFold, Random Forest, 5000 as minimum number of trees,SMOTE as a balance technique, and 10% as split size. You can change:
1. RF to SVM, DT, MLP, NB or KNN
2. StratifiedKFold to KFold
3. SMOTE to none (to see the result without balancing)
4. Any number of trees, IG, or split size


-- still need improve.. 
